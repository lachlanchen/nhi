\documentclass[9pt,twocolumn]{opticajnl}
\journal{opticajournal}
\setboolean{shortarticle}{true}

% \title{Self-synchronized event-driven transmission hyperspectral imaging with multi-window time-warp compensation}
\title{A Low-cost Self-Calibrated Neuromorphic Hyperspectral Imaging Framework}
\author[1]{Rongzhou Chen}
\author[1]{Edmund Y. Lam}
\affil[1]{Department of Electrical and Electronic Engineering, The University of Hong Kong}
\dates{\today}
\ociscodes{Multispectral and hyperspectral imaging; Instrumentation, measurement, and metrology; Image reconstruction techniques; Computational imaging}
\doi{\url{}}

\begin{abstract}
We demonstrate a visible-band transmission hyperspectral system that drives a diffraction-scanned illumination across a specimen and records the induced intensity dynamics with an event camera. Instead of relying on calibrated motor encoders or trigger wiring, the method self-synchronizes the scan by analysing the event activity via auto- and self-cross-correlation, yielding forward/backward segmentation directly from the data stream. A piecewise-affine, multi-window time-warp model with soft spatial memberships then compensates acceleration-induced temporal shear by minimizing within-bin event variance, outperforming single-slope corrections under low-cost motor actuation. The reconstructed spectra match a reference hyperspectral camera even in low-light conditions where the frame-based sensor degrades, underlining the advantage of the event-driven capture. 
\end{abstract}

\usepackage{ifthen}
\usepackage{placeins}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning}

% Yellow placeholder figure macro for drafts
\newcommand{\phfig}[2][0.95\linewidth]{%
  \begingroup\setlength{\fboxsep}{0pt}\colorbox{yellow!20}{%
    \parbox{#1}{\vspace{16mm}\centering\textbf{PLACEHOLDER}\\\small #2\vspace{14mm}}}%
  \endgroup}

\providecommand{\nocitefullrefs}[1]{}

\begin{document}

\maketitle

\section{Introduction}
Scanning hyperspectral imagers deliver high spectral resolution at the expense of mechanical complexity and strict synchronization between motion and detection elements~\cite{hagen2013review}. Conventional systems integrate encoder signals, tachometers, or trigger lines to maintain spatial--spectral registration, which complicates low-cost deployments and long-term stability~\cite{yeh2011selfcal,alhourani2023linescan}. Event cameras provide an alternative sensing modality: they asynchronously report logarithmic intensity changes with microsecond latency and high dynamic range, recording only the temporal variations induced by a spectral scan~\cite{gallego2020event}. Prior event-driven hyperspectral work has typically relied on actively modulated illumination or precisely calibrated scanning optics to ensure temporal alignment~\cite{reinbacher2016manifold,bardow2016simultaneous}. Here we pursue a self-synchronized architecture that accepts inexpensive, non-uniform motor motion and restores spectral structure algorithmically.

The system operates in transmission across the visible band (420--700~nm), reducing surface-scatter artefacts and enabling a tractable model that links event timing to wavelength. The key technical contributions are: (i) an event-domain auto-/self-cross-correlation strategy that segments forward and backward scans without auxiliary sensors; (ii) a multi-window, soft-membership time-warp compensation that minimizes within-bin variance to undo acceleration-induced temporal shear, extending piecewise-affine warps from event-motion studies~\cite{mitrokhin2018mobility}; and (iii) a reconstruction pipeline that integrates polarity-balanced events into spectral estimates, benchmarking them against a frame-based hyperspectral reference in both nominal and low-light regimes. 
% Planned experiments include the benefits of single-pass versus multi-pass fusion and assess robustness under reduced illumination, complementing the figure plan outlined below.

\section{System architecture and sensing model}
Figure~\ref{fig:overview}a sketches the optical layout. A broadband LED illuminates a specimen; a diffraction grating mounted in a 4$f$ relay is driven by a low-cost galvanometer or stepper stage to sweep the dispersed spectrum across a hybrid sensor plane that houses a dynamic vision sensor (DVS) and a co-aligned frame imager. The motor executes repeated forward/backward scans (currently three in each direction), but its speed is neither constant nor calibrated.

Under small-angle assumptions the wavelength incident on pixel $(x,y)$ is
\begin{equation}
  \lambda(x,t) \approx d\!\left(\frac{x}{z_2} - \frac{\xi(t)}{z_1}\right),
  \label{eq:lambda_mapping}
\end{equation}
where $d$ is the grating dispersion constant, $z_2$ and $z_1$ denote distances from the grating to the sensor and illumination relay, and $\xi(t)$ is the motor displacement inferred from the data stream. The transmitted intensity can be written as $I_{x,y}(t) = I_0^{(x,y)}[\lambda(x,t)]$, leading to the temporal gradient
\begin{equation}
  \frac{\partial I_{x,y}}{\partial t} = \frac{\partial I_0^{(x,y)}}{\partial \lambda} \frac{\partial \lambda(x,t)}{\partial t},
  \label{eq:intensity_gradient}
\end{equation}
which drives event emission when the logarithmic intensity crosses the sensor threshold. Because $\partial \lambda/\partial t$ is governed by the motor trajectory, reconstructing $I_0^{(x,y)}(\lambda)$ reduces to integrating polarity-weighted events along a synchronized wavelength axis.

\FloatBarrier
\begingroup\setlength{\abovecaptionskip}{2pt}\setlength{\belowcaptionskip}{0pt}
\begin{figure*}[!t]
  \centering
  \includegraphics[width=0.7\linewidth]{../../publication_code/figures/figure01_overview.pdf}
  \caption{System overview and modular integration. A dispersed illumination path (left, blue dashed box) acts as a drop-in module before the sample, while a non-intrusive detection add-on (right, green dashed box) uses a 4$f$ relay to feed an event+frame sensor. An optional splitter or camera port preserves the original microscope camera.}
  \label{fig:overview}
\end{figure*}
\endgroup

\section{Event-trigger probability model}
Event emission occurs when the logarithmic brightness increment surpasses a fixed magnitude $\theta$. Under realistic sensing conditions the observed gradient is corrupted by noise, which we model as
\begin{equation}
  \nabla I = \nabla I_{\text{true}} + \eta,\qquad \eta \sim \mathcal{N}(0,\sigma^2).
  \label{eq:gradient_noise}
\end{equation}
The probability of a positive event equals the likelihood that the noise-perturbed gradient exceeds the threshold,
\begin{equation}
  P(\text{event}) = P(\eta > \theta - \nabla I_{\text{true}}) = \tfrac12\!\left[1 - \operatorname{erf}\!\left(\frac{\theta - \nabla I_{\text{true}}}{\sqrt{2}\sigma}\right)\right],
  \label{eq:trigger_probability}
\end{equation}
with an analogous form for negative events around $-\theta$. Approximating the error function by a logistic curve yields the computationally convenient sigmoid expression
\begin{equation}
  P(\text{event}) \approx \sigma\!\left(\frac{\nabla I_{\text{true}} - \theta}{\sigma'}\right),\qquad \sigma' \approx \frac{2\sqrt{2}}{\sqrt{\pi}}\sigma,
  \label{eq:sigmoid_probability}
\end{equation}
where $\sigma(\cdot)$ denotes the logistic sigmoid. Within a temporal bin of duration $\Delta t$ the expected event count relates directly to this probability,
\begin{equation}
  \langle N_{\text{evt}}\rangle = \lambda\,P(\text{event})\,\Delta t,
  \label{eq:expected_events}
\end{equation}
with $\lambda$ the effective sampling rate per pixel. Averaging events over repeated scans gives the empirical mean $\bar{N}_{\text{evt}}$, which can be inverted via the logit function $\sigma^{-1}$ to estimate the true gradient:
\begin{equation}
  \nabla I_{\text{est}} = \theta + \sigma'\,\sigma^{-1}\!\left(\frac{\bar{N}_{\text{evt}}}{\lambda\,\Delta t}\right).
  \label{eq:gradient_estimate}
\end{equation}

This probabilistic link between event statistics and intensity gradients underpins our reconstruction strategy. Equation~(\ref{eq:gradient_estimate}) justifies using temporally binned events as sufficient statistics for spectral estimation: the scan-induced evolution of $\nabla I_{\text{true}}$ encodes spectral slopes, while averaging suppresses sensor noise and preserves the dynamic-range benefits of event vision. The next sections leverage these statistics to segment scans and correct the residual temporal shear before building the spectral cube.

\section{Self-synchronized scan segmentation}
We first convert the event timestamps into an activity signal $a[n]$ (events per 1~ms bin). A sliding window search identifies the densest subsequence containing at least 80\% of all events, yielding an initial round-trip estimate. The auto-correlation
\begin{equation}
  R[k] = \sum_{n} a[n]\,a[n+k]
\end{equation}
reveals the dominant period, while the reverse-correlation between $a[n]$ and $a[N{-}1{-}n]$ sharpens the turnaround index by exploiting mirror symmetry between forward and backward sweeps. An iterative peak search refines the start and end indices of each half-scan, producing six segments (three forward, three backward) without reference measurements. Anti-correlation further labels scan direction by comparing the sign of slope changes surrounding detected peaks.

This data-driven segmentation adapts to slow drifts or perturbations, ensuring that subsequent reconstructions operate on consistent temporal support. 
% Planned figs include (i) using only the first forward/back pair to quantify baseline noise, and (ii) averaging all three pairs to demonstrate variance reduction. 

\begin{figure}[t]
  \centering
  \begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (f2a) at (0,0)
      {\includegraphics[width=\linewidth]{../../publication_code/figures/figure02_activity.pdf}};
    % \begin{scope}[x={(f2a.south east)}, y={(f2a.north west)}]
    %   \node[font=\bfseries, anchor=north west, fill=none, text=black, inner sep=2pt] at (0.012, 0.988) {(a)};
    % \end{scope}
  \end{tikzpicture}\\[-2pt]
  \begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (f2b) at (0,0)
      {\includegraphics[width=\linewidth]{../../publication_code/figures/figure02_correlation.pdf}};
    % \begin{scope}[x={(f2b.south east)}, y={(f2b.north west)}]
    %   \node[font=\bfseries, anchor=north west, fill=none, text=black, inner sep=2pt] at (0.012, 0.988) {(b)};
    % \end{scope}
  \end{tikzpicture}
  \vspace{-3pt}
  \caption{Self-synchronized scan segmentation. (a) Event activity over the full recording with pre-/post-scan shading and alternating forward/backward boundaries. (b) Auto- and reverse-correlation of the activity signal; the blue marker indicates the mirror-peak used to localize the turnaround.}
  \label{fig:segmentation}
\end{figure}

\section{Multi-window time-warp compensation}
Even with accurate segmentation, \eqref{eq:lambda_mapping} introduces a spatially varying delay through the $x/z_2$ term; motor acceleration near turnarounds further shears the time axis. Our multi-window compensation routine addresses this by fitting boundary surfaces
\begin{equation}
  T_i(x,y) = a_i x + b_i y + c_i,
\end{equation}
which partition each scan into $M$ temporal windows. Soft memberships,
\begin{equation}
  w_i(x,y,t) = \frac{\sigma\!\left(\frac{t - T_i}{\tau}\right)\sigma\!\left(\frac{T_{i+1}-t}{\tau}\right)}{\sum_j \sigma\!\left(\frac{t - T_j}{\tau}\right)\sigma\!\left(\frac{T_{j+1}-t}{\tau}\right)},
\end{equation}
blend adjacent windows (temperature $\tau \approx 1$~ms). The compensated time is
\begin{equation}
  t' = t - \sum_{i=1}^{M-1} w_i(x,y,t)\bigl(\tilde{a}_i x + \tilde{b}_i y \bigr),
  \label{eq:timewarp}
\end{equation}
where $\{\tilde{a}_i,\tilde{b}_i\}$ are optimized by minimizing the spatial variance of event counts within short $t'$ bins while penalizing abrupt parameter changes between neighbouring windows. Gradient-based updates use mini-batches of events to respect GPU memory limits. Compared with a single-plane warp (global $a,b$), the multi-window model better captures acceleration and deceleration phases, yielding sharper reconstructions.

Figure~\ref{fig:warp} will visualise diagnostics: (a) $x$--$t$ projections before compensation showing diagonal striations; (b) the same after applying \eqref{eq:timewarp}; (c) cumulative event frames before and after warping, highlighting blur reduction; and (d) the learned boundary curves $T_i(x)$ that reveal the motor’s velocity profile. The variance reductions and residual blur metrics contrasts the multi-window and single-plane models.

\paragraph{Objective.} Let $\theta=\{\tilde a_i,\tilde b_i\}_{i=1}^{M-1}$ denote the trainable time-warp slopes (and, when enabled, boundary offsets $\{c_i\}$). For events $\{(x_n,y_n,t_n,p_n)\}_{n=1}^N$ and compensated times $t'_n$ from \eqref{eq:timewarp}, we accumulate polarity-weighted counts into short bins of width $\Delta t$ around $t'_n$ using linear time interpolation. Writing $\phi_k(\cdot)$ for the hat function that distributes mass to the two nearest bin centres,
\begin{equation}
  \mathsf{E}_k(x,y;\theta) \,=\, \sum_{n=1}^N p_n\,\phi_k\!\bigl(t'_n\bigr)\,\mathbf{1}[x_n{=}x,\,y_n{=}y],
  \label{eq:binning}
\end{equation}
we minimize the sum of spatial variances across all bins,
\begin{equation}
  \mathcal{L}_{\mathrm{var}}(\theta)
  \,= \sum_k \operatorname{Var}_{x,y}\bigl[\mathsf{E}_k(x,y;\theta)\bigr]
  \,= \sum_k \frac{1}{HW}\sum_{x,y}\bigl(\mathsf{E}_k(x,y;\theta)-\mu_k\bigr)^2,
  \label{eq:variance_loss}
\end{equation}
where $H\times W$ is the sensor size and $\mu_k = (HW)^{-1}\sum_{x,y}\mathsf{E}_k(x,y;\theta)$. A quadratic smoothness prior couples neighbouring windows to discourage abrupt parameter changes,
\begin{equation}
  \mathcal{L}_{\mathrm{sm}}(\theta) 
  \,= \sum_{i=1}^{M-2} \Big[(\tilde a_{i+1}{-}\tilde a_i)^2 + (\tilde b_{i+1}{-}\tilde b_i)^2\Big]
  \,+ \;\mathbf{1}_{\mathrm{boundaries}}\sum_{i}(c_{i+1}{-}c_i)^2,
  \label{eq:smoothness}
\end{equation}
where the last term is included only when boundary offsets are trainable. The full objective is
\begin{equation}
  \mathcal{L}(\theta) \,= \mathcal{L}_{\mathrm{var}}(\theta) + \lambda_{\mathrm{sm}}\,\mathcal{L}_{\mathrm{sm}}(\theta),
  \label{eq:total_loss}
\end{equation}
and is minimized by gradient descent with backpropagation through the differentiable memberships and splatting in \eqref{eq:binning}. In practice we stream events in chunks to form $\mathsf{E}_k$ but always accumulate a single unified tensor to evaluate \eqref{eq:variance_loss}, matching the implementation used for training.

\subsection*{Time--wavelength self-alignment}
Let $b(t)$ denote the 1D background series computed from polarity-weighted, fast-compensated events with a fixed step $\Delta t$ (5~ms in our experiments). We first normalise $b$ by the start/end plateaus to obtain
\begin{equation}
  \tilde b(t) \,=\, \frac{b(t)-\mu_{\mathrm{pre}}}{\mu_{\mathrm{post}}-\mu_{\mathrm{pre}}},\quad t\in[0,T],
\end{equation}
where $\mu_{\mathrm{pre}}$ and $\mu_{\mathrm{post}}$ are means over small windows at the beginning and end of the scan, respectively. After smoothing $\tilde b$ by a short moving average, we detect the active interval $[t_0,t_1]$ as the maximal open interval whose values exceed a fixed fraction of the dynamic range (equivalently, the two largest slope changes).

Let $\{g_i(\lambda)\}_{i=1}^M$ be spectrometer ground-truth curves, normalised per-curve to $\tilde g_i$ by the same plateau procedure, and let $[\lambda_{0,i},\lambda_{1,i}]$ denote their active intervals. We estimate the time$\to$wavelength map as the unique affine transform
\begin{equation}
  \lambda(t) \,=\, \alpha t + \beta,\qquad
  \alpha \,=\, \frac{\bar\lambda_1-\bar\lambda_0}{t_1-t_0},\;\;\beta \,=\, \bar\lambda_0 - \alpha t_0,
  \label{eq:time_to_wavelength}
\end{equation}
with $\bar\lambda_0=\tfrac{1}{M}\sum_i \lambda_{0,i}$ and $\bar\lambda_1=\tfrac{1}{M}\sum_i \lambda_{1,i}$. Mapping $\tilde b$ through \eqref{eq:time_to_wavelength} yields $\tilde b_\lambda(\lambda){=}\tilde b\big((\lambda{-}\beta)/\alpha\big)$.

For plotting or optional amplitude matching, we compute a least-squares affine fit of $\tilde b_\lambda$ to the mean ground-truth $\bar g(\lambda){=}\tfrac{1}{M}\sum_i \tilde g_i(\lambda)$ on a common grid $\{\lambda_j\}_{j=1}^N$ by
\begin{equation}
  (a^*,c^*) \,=\, \arg\min_{a,c} \sum_{j=1}^N\!\big(\bar g(\lambda_j)-a\,\tilde b_\lambda(\lambda_j)-c\big)^2,
  \label{eq:amplitude_fit}
\end{equation}
whose closed-form solution is $[a^*,c^*]^\top=(X^\top X)^{-1}X^\top\!\mathbf{y}$ with $X=[\tilde b_\lambda\;\;\mathbf{1}]$ and $\mathbf{y}=\bar g$.

\begin{figure}[t]
  \centering
  % Figure 3(a): Events with learned boundaries (X--T and Y--T)
  \begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (img3a) at (0,0)
      {\includegraphics[width=\linewidth]{../../publication_code/figures/figure03_a_events.pdf}};
    \begin{scope}[x={(img3a.south east)}, y={(img3a.north west)}]
      \node[font=\bfseries, anchor=north west, fill=none, text=black, inner sep=2pt] at (0.006, 1.030) {(a)};
    \end{scope}
  \end{tikzpicture}\\[1pt]
  % Figure 3(b): Variance vs time (50 ms bins)
  \begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (img3b) at (0,0)
      {\includegraphics[width=\linewidth]{../../publication_code/figures/figure03_b_variance.pdf}};
    \begin{scope}[x={(img3b.south east)}, y={(img3b.north west)}]
      \node[font=\bfseries, anchor=north west, fill=none, text=black, inner sep=2pt] at (0.006, 1.030) {(b)};
    \end{scope}
  \end{tikzpicture}\\[1pt]
  % Figure 3(c): Single 50 ms bin comparison (original vs compensated)
  \begin{tikzpicture}
    \node[anchor=south west, inner sep=0] (img3c) at (0,0)
      {\includegraphics[width=\linewidth]{../../publication_code/figures/figure03_c_bin50ms.pdf}};
    \begin{scope}[x={(img3c.south east)}, y={(img3c.north west)}]
      \node[font=\bfseries, anchor=north west, fill=none, text=black, inner sep=2pt] at (0.006, 1.030) {(c)};
    \end{scope}
  \end{tikzpicture}
  \caption{Multi-window compensation diagnostics. (a) Events (X--T and Y--T) with learned boundary surfaces overlaid. (b) Variance per 50~ms bin over time (original vs. compensate). (c) Example 50~ms bin comparing original and compensate frames.}
  \label{fig:warp}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\textwidth]{../../publication_code/figures/figure04_allinone_20251101_161009/figure04_rescaled_grid_bins_03_15.pdf}
  \caption{
  Spectral reconstruction across a single forward scan. Each column shows a 50~ms temporal bin.
   % (top: original; bottom: compensated with 3$\times$3$\times$3 smoothing and background subtraction). 
   }
  \label{fig:spectrum}
\end{figure*}

\begin{figure}[t]
  \centering
  % Single aligned overlay (Background vs Groundtruth)
  \includegraphics[width=\columnwidth]{../../publication_code/figures/figure04_allinone_20251103_214813/figure04_rescaled_bg_gt_third_only.png}
  \caption{Aligned background (\emph{Background}) mapped to wavelength using the self-alignment in \eqref{eq:time_to_wavelength} compared against a single spectrometer curve (\emph{Groundtruth}). Both traces are normalised by plateau levels; no truncation is applied.}
  \label{fig:figure5}
\end{figure}

\section{Spectral reconstruction and benchmarking}
After compensation we bin events along $t'$ to obtain quasi-static frames $\mathsf{E}_k(x,y)$ over 1--2~ms intervals, integrate polarity-weighted counts to recover relative spectra. Three forward/backward cycles are currently fused to suppress noise, but the segmentation framework enables rigorous evaluation of single-cycle reconstructions. Ground-truth comparison uses a frame-based hyperspectral camera capturing the same scene; spectral overlays and residual maps quantify accuracy. Low-light trials---achieved by dimming the LED source---highlight the event camera’s high dynamic range: the reference system exhibits elevated noise and requires longer exposures, whereas the event reconstruction remains stable.

Figure~\ref{fig:spectrum} outlines the intended presentation: (a) reconstructed monochrome slices at representative wavelengths; (b) an RGB composite; (c) spectral curves comparing event reconstruction, reference camera, and residuals; and (d) a quantitative panel showing low-light error bars or forward-only versus multi-pass fusion performance. 


\section{Discussion and outlook}
The self-synchronizing strategy removes the need for encoder calibration while tolerating speed fluctuations from cost-effective actuators, distinguishing this platform from prior systems that assumed precise timing hardware~\cite{hagen2013review,yeh2011selfcal}. The variance-minimizing, multi-window time warp extends piecewise-affine compensations used in event-based motion estimation~\cite{mitrokhin2018mobility}, adapting them to the spectral scanning context. Together these components enable transmission-mode spectral capture without illumination modulation or auxiliary sensors.

% Planned figures: (i) gather datasets spanning single and triple scan fusion to populate Fig.~\ref{fig:segmentation}(d); (ii) quantify variance reduction and residual blur for multi-window versus single-plane warps to complete Fig.~\ref{fig:warp}; and (iii) collect spectral overlays under nominal and low-light scenarios to fill Fig.~\ref{fig:spectrum}. 

\section{Conclusion}
We presented a self-synchronized, event-driven transmission hyperspectral system that derives scan timing from the event stream and compensates temporal shear using a multi-window time warp. Despite relying on inexpensive, non-uniform motor motion, the reconstructed spectra align with a reference hyperspectral camera and retain fidelity under reduced illumination. 

\bibliography{ref}

\end{document}
